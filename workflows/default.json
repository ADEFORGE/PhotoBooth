{
  "3": {
    "inputs": {
      "control_net_name": "1.5\\control_v11f1p_sd15_depth_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "7": {
    "inputs": {
      "images": [
        "23",
        5
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "9": {
    "inputs": {
      "upscale_model": [
        "14",
        0
      ],
      "image": [
        "23",
        5
      ]
    },
    "class_type": "ImageUpscaleWithModel",
    "_meta": {
      "title": "Upscale Image (using Model)"
    }
  },
  "12": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "9",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "14": {
    "inputs": {
      "model_name": "RealESRGAN_x2.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "16": {
    "inputs": {
      "image": [
        "17",
        0
      ]
    },
    "class_type": "GetImageSize",
    "_meta": {
      "title": "Get Image Size"
    }
  },
  "17": {
    "inputs": {
      "image": "WIN_20250715_14_18_05_Pro.jpg"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "19": {
    "inputs": {
      "ckpt_name": "dreamshaper_8.safetensors",
      "vae_name": "Baked VAE",
      "clip_skip": -1,
      "lora_name": "None",
      "lora_model_strength": 1.0000000000000002,
      "lora_clip_strength": 1.0000000000000002,
      "positive": [
        "88",
        0
      ],
      "negative": "embedding:BadDream, embedding:UnrealisticDream, embedding:FastNegativeV2, (bare chest, breasts, visible anatomy, nude:1), ugly, deformed, noisy, blurry, low contrast, modern clothing, plastic, low detail, extra limbs, blurry, distorted face, text, watermark, poorly drawn hands, naked, muscles, nsfw",
      "token_normalization": "mean",
      "weight_interpretation": "comfy",
      "empty_latent_width": [
        "20",
        0
      ],
      "empty_latent_height": [
        "21",
        0
      ],
      "batch_size": 1,
      "cnet_stack": [
        "26",
        0
      ]
    },
    "class_type": "Efficient Loader",
    "_meta": {
      "title": "Efficient Loader"
    }
  },
  "20": {
    "inputs": {
      "expression": "a//b\n\n",
      "a": [
        "16",
        0
      ],
      "b": [
        "71",
        0
      ]
    },
    "class_type": "MathExpression|pysssss",
    "_meta": {
      "title": "Math Expression üêç"
    }
  },
  "21": {
    "inputs": {
      "expression": "a//b",
      "a": [
        "16",
        1
      ],
      "b": [
        "71",
        0
      ]
    },
    "class_type": "MathExpression|pysssss",
    "_meta": {
      "title": "Math Expression üêç"
    }
  },
  "23": {
    "inputs": {
      "seed": 1089789284519586,
      "steps": 15,
      "cfg": 3.6,
      "sampler_name": "dpmpp_2m",
      "scheduler": "AYS SD1",
      "denoise": 1,
      "preview_method": "taesd",
      "vae_decode": "true",
      "model": [
        "93",
        0
      ],
      "positive": [
        "19",
        1
      ],
      "negative": [
        "19",
        2
      ],
      "latent_image": [
        "19",
        3
      ],
      "optional_vae": [
        "19",
        4
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "26": {
    "inputs": {
      "strength": 0.5000000000000001,
      "start_percent": 0,
      "end_percent": 0.8000000000000002,
      "control_net": [
        "3",
        0
      ],
      "image": [
        "45",
        0
      ],
      "cnet_stack": [
        "47",
        0
      ]
    },
    "class_type": "Control Net Stacker",
    "_meta": {
      "title": "Control Net Stacker"
    }
  },
  "45": {
    "inputs": {
      "preprocessor": "DepthAnythingV2Preprocessor",
      "resolution": [
        "16",
        0
      ],
      "image": [
        "17",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "47": {
    "inputs": {
      "strength": 0.5000000000000001,
      "start_percent": 0,
      "end_percent": 0.8000000000000002,
      "control_net": [
        "48",
        0
      ],
      "image": [
        "98",
        0
      ]
    },
    "class_type": "Control Net Stacker",
    "_meta": {
      "title": "Control Net Stacker"
    }
  },
  "48": {
    "inputs": {
      "control_net_name": "1.5\\control_v11e_sd15_ip2p_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "71": {
    "inputs": {
      "value": 2.1
    },
    "class_type": "PrimitiveFloat",
    "_meta": {
      "title": "Float"
    }
  },
  "85": {
    "inputs": {
      "images": [
        "17",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "87": {
    "inputs": {
      "images": [
        "45",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "88": {
    "inputs": {
      "text": "Turn this person into a marble statue while preserving their original identity, age, and gender exactly. Sculpt them in smooth, white marble with realistic but modest features. Avoid any exaggeration or changes to body shape or facial structure. The statue should resemble classical art with respectful, child-appropriate detail. Use soft lighting and natural stone texture, with no added accessories or fantasy elements. Make sure the whole statue is white without colors"
    },
    "class_type": "Text Multiline",
    "_meta": {
      "title": "Text Multiline"
    }
  },
  "92": {
    "inputs": {
      "preset": "FACEID PORTRAIT (style transfer)",
      "lora_strength": 0.6000000000000001,
      "provider": "CPU",
      "model": [
        "19",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoaderFaceID",
    "_meta": {
      "title": "IPAdapter Unified Loader FaceID"
    }
  },
  "93": {
    "inputs": {
      "weight": 1.0000000000000002,
      "weight_faceidv2": 1.0000000000000002,
      "weight_type": "composition",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "92",
        0
      ],
      "ipadapter": [
        "92",
        1
      ],
      "image": [
        "17",
        0
      ]
    },
    "class_type": "IPAdapterFaceID",
    "_meta": {
      "title": "IPAdapter FaceID"
    }
  },
  "96": {
    "inputs": {
      "preprocessor": "none",
      "resolution": [
        "16",
        0
      ],
      "image": [
        "17",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "97": {
    "inputs": {
      "images": [
        "96",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "98": {
    "inputs": {
      "brightness": 0,
      "contrast": 1,
      "saturation": 0,
      "sharpness": 1,
      "blur": 0,
      "gaussian_blur": 0,
      "edge_enhance": 0,
      "detail_enhance": "false",
      "image": [
        "96",
        0
      ]
    },
    "class_type": "Image Filter Adjustments",
    "_meta": {
      "title": "Image Filter Adjustments"
    }
  },
  "99": {
    "inputs": {
      "images": [
        "98",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  }
}